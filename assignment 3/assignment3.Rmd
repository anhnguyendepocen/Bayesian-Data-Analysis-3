---
title: "Bayesian Data Analysis - Assignment 3"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 2
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load `aaltobda` library and `winsdhield` data.
```{r}
library(aaltobda)
data("windshieldy1")
data("windshieldy2")
set.seed(4711)
```

## 1. Inference for normal mean and deviation
Observed hardness values $\mathbf{y}_1$. Assumed to be normally distributed with unknown standard deviation $\sigma$ and unknown average hardness $\mu.$ We formulate our Baysian model with noninformative prior. The sufficient statistics are sample mean $\overline{y}=\frac{1}{n}\sum_{i=1}^n y_i$ and sample variance $s^2 = \frac{1}{n-1}\sum_{i=1}^n (y_i - \overline{y})^2.$ In R they are computed using `mean` and `var` respectively. By itself, $s$ denotes standard deviation. In R, it's computed using `sd`.

As the source for the equations for the prior, likelihood, and posterior, we refer to @bayesian_data_analysis_2013, chapters 3.1 - 3.2.

### a)
The posterior distribution is Stundent's $t$ ditribution of form
$$
\mu\mid y \sim t_{n-1}(\overline{y}, s/\sqrt{n})
$$

Bayesian point estimate: Expected value of conditional posterior distribution, sample mean
$$
E(p(\mu\mid y))=\overline{y}
$$

```{r}
mu_point_est <- function(data) {
  mean(data)
}
mu_point_est(windshieldy1)
```

Bayesian interval estimate:
```{r}
mu_interval <- function(data, prob) {
  n = length(data)
  y = mean(data)
  s = sd(data)
  scale = s/sqrt(n)
  df = n - 1
  a = (1 - prob)/2
  scale * qt(c(a, a+prob), df) + y
}
mu_interval(windshieldy1, 0.95)
```

Density plot
```{r}
data = windshieldy1
n = length(data)
y = mean(data)
s = sd(data)
scale = s/sqrt(n)
x = seq(y-2*s^2, y+2*s^2, 0.1)
df = n - 1
d = dtnew(x, df, y, scale)
plot(x, d, type = "l")
```

### b)
The predictive posterior distribution is Student's $t$ ditribution of form
$$
\tilde{y}\mid y \sim t_{n-1}\left(\overline{y}, s\sqrt{1+\frac{1}{n}}\right)
$$

Bayesian point estimate: Expected value of posterior predictive distribution 
$$
E(p(\tilde{y}\mid y))=\overline{y}
$$
```{r}
mu_pred_point_est <- function(data) {
  mean(data)
}
mu_pred_point_est(windshieldy1)
```

Bayesian interval estimate: 
```{r}
mu_pred_interval <- function(data, prob) {
  n = length(data)
  y = mean(data)
  s = sd(data)
  scale = sqrt(1 + 1/n) * s
  a = (1 - prob)/2
  scale * qt(c(a, a+prob), df = n - 1) + y
}
mu_pred_interval(windshieldy1, 0.95)
```

Density plot
```{r}
data = windshieldy1
n = length(data)
y = mean(data)
s = sd(data)
scale = sqrt(1 + 1/n) * s
x = seq(y-2*s^2, y+2*s^2, 0.1)
df = n - 1
d = dtnew(x, df, y, scale)
plot(x, d, type = "l")
```


## 2. Inference for the difference between proportions
As the source for the equations for the prior, likelihood, and posterior, we refer to @bayesian_data_analysis_2013, chapter 2.1.

The likelihood is binomial distribution and prior beta distribution.
```{r}
# Control
n0 = 674
y0 = 39
# Treatment
n1 = 680
y1 = 22
```

Posterior distribution is Beta distribution.
$$
\operatorname{Beta}(y+1, n-y+1)
$$

```{r}
m = 100000
p0 <- rbeta(m, y0 + 1, n0 - y0 + 1)
p1 <- rbeta(m, y1 + 1, n1 - y1 + 1)
```

The odds ratio is defined
```{r}
odds_ratio <- function(p0, p1) {
  (p1/(1-p1))/(p0/(1-p0))
}
```

### a)
Point estimate
```{r}
posterior_odds_ratio_point_est <- function(p0, p1) {
  mean(odds_ratio(p0, p1))
}
posterior_odds_ratio_point_est(p0, p1)
```

Interval estimate
```{r}
posterior_odds_ratio_interval <- function(p0, p1, prob) {
  a = (1-prob)/2
  quantile(odds_ratio(p0, p1), c(a, a+prob))
}
posterior_odds_ratio_interval(p0, p1, prob = 0.9)
```

Histogram
```{r}
hist(odds_ratio(p0, p1), breaks = 100)
```

### b)


## 3. Inference for the difference between normal means
### a)
Define a new random variable as the difference between the average hardnesses
$$
\mu_d = \mu_1 - \mu_2
$$

For simulating the random variable $\mu_i$ we use the same Student's $t$ distribution as in exercise 1a
```{r}
r_mu <- function(m, data) {
  n = length(data)
  y = mean(data)
  s = sd(data)
  scale = s/sqrt(n)
  scale * rt(m, df = n - 1) + y
}
```

Histogram
```{r}
m = 100000
mu_d = r_mu(m, windshieldy1) - r_mu(m, windshieldy2)
hist(mu_d, breaks = 100)
```

Point estimate
```{r}
mean(mu_d)
```

Interval estimate $95\%$
```{r}
quantile(mu_d, c(0.025, 0.975))
```

### b)
The means are different. This is evident from the point estimate which is not close to zero and interval estimate is skewed towards negative values. The histogram also visualizes that the probability mass is mostly located on the negative side.


## References
